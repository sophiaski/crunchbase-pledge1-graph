{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Pipeline\n",
    "\n",
    "**Can we improve on the baseline scores with added features, imputation completed, and MinMax scaling?**\n",
    "\n",
    "**`p1_tag` ~  `rank` + `employee_count` (ordinal) +  `total_funding_usd` + `age` + `continent` (nominal) + `industry` (nominal)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'graph' environment to PATH\n",
    "import sys\n",
    "sys.path.append('/home/ski/anaconda3/envs/graph/lib/python3.8/site-packages')\n",
    "\n",
    "# User defined functions\n",
    "import base_methods\n",
    "from importlib import reload\n",
    "from base_methods import load_the_csvs\n",
    "\n",
    "# Import data analysis packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML\n",
    "import category_encoders as ce\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/ski/Desktop/crunchbase-p1-machine-learning/files/output/’: File exists\n",
      "\n",
      "/HOME/SKI/DESKTOP/CRUNCHBASE-P1-MACHINE-LEARNING/FILES/OUTPUT/BASELINE_IMPUTE_COMPLETE.CSV\n",
      "BASELINE_IMPUTE_COMPLETE shape: (1131325, 61)\n",
      "BASELINE_IMPUTE_COMPLETE columns: ['uuid', 'p1_tag', 'country_code', 'category_groups_list', 'continent_code', 'employee_count', 'total_funding_usd', 'rank', 'age', 'ind_1', 'ind_2', 'ind_3', 'ind_4', 'ind_5', 'ind_6', 'ind_7', 'ind_8', 'ind_9', 'ind_10', 'ind_11', 'ind_12', 'ind_13', 'ind_14', 'ind_15', 'ind_16', 'ind_17', 'ind_18', 'ind_19', 'ind_20', 'ind_21', 'ind_22', 'ind_23', 'ind_24', 'ind_25', 'ind_26', 'ind_27', 'ind_28', 'ind_29', 'ind_30', 'ind_31', 'ind_32', 'ind_33', 'ind_34', 'ind_35', 'ind_36', 'ind_37', 'ind_38', 'ind_39', 'ind_40', 'ind_41', 'ind_42', 'ind_43', 'ind_44', 'ind_45', 'ind_46', 'cont_AF', 'cont_AS', 'cont_EU', 'cont_NA', 'cont_OC', 'cont_SA']\n",
      "\n",
      "\n",
      "Ending Dataframe Columns:\n",
      "\n",
      "['p1_tag', 'employee_count', 'total_funding_usd', 'rank', 'age', 'ind_1', 'ind_2', 'ind_3', 'ind_4', 'ind_5', 'ind_6', 'ind_7', 'ind_8', 'ind_9', 'ind_10', 'ind_11', 'ind_12', 'ind_13', 'ind_14', 'ind_15', 'ind_16', 'ind_17', 'ind_18', 'ind_19', 'ind_20', 'ind_21', 'ind_22', 'ind_23', 'ind_24', 'ind_25', 'ind_26', 'ind_27', 'ind_28', 'ind_29', 'ind_30', 'ind_31', 'ind_32', 'ind_33', 'ind_34', 'ind_35', 'ind_36', 'ind_37', 'ind_38', 'ind_39', 'ind_40', 'ind_41', 'ind_42', 'ind_43', 'ind_44', 'ind_45', 'ind_46', 'cont_AF', 'cont_AS', 'cont_EU', 'cont_NA', 'cont_OC', 'cont_SA']\n",
      "\n",
      "Dataframe shape: (1131325, 57)\n"
     ]
    }
   ],
   "source": [
    "# Store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]\n",
    "\n",
    "# Set global paths to data folders\n",
    "!mkdir {PWD}/files/output/\n",
    "print()\n",
    "INPUT = PWD + '/files/csv/'\n",
    "OUTPUT = PWD + '/files/output/'\n",
    "\n",
    "# Load\n",
    "df = load_the_csvs(loc=OUTPUT, data=['baseline_impute_complete'], verbose=True)\n",
    "\n",
    "# Remove columns not used in model\n",
    "df_simple = df.drop(['country_code','continent_code','category_groups_list','uuid'], axis=1)\n",
    "\n",
    "print('\\nEnding Dataframe Columns:\\n\\n{}'.format(df_simple.columns.to_list()))\n",
    "print('\\nDataframe shape:', df_simple.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: ['employee_count', 'total_funding_usd', 'rank', 'age', 'ind_1', 'ind_2', 'ind_3', 'ind_4', 'ind_5', 'ind_6', 'ind_7', 'ind_8', 'ind_9', 'ind_10', 'ind_11', 'ind_12', 'ind_13', 'ind_14', 'ind_15', 'ind_16', 'ind_17', 'ind_18', 'ind_19', 'ind_20', 'ind_21', 'ind_22', 'ind_23', 'ind_24', 'ind_25', 'ind_26', 'ind_27', 'ind_28', 'ind_29', 'ind_30', 'ind_31', 'ind_32', 'ind_33', 'ind_34', 'ind_35', 'ind_36', 'ind_37', 'ind_38', 'ind_39', 'ind_40', 'ind_41', 'ind_42', 'ind_43', 'ind_44', 'ind_45', 'ind_46', 'cont_AF', 'cont_AS', 'cont_EU', 'cont_NA', 'cont_OC', 'cont_SA']\n",
      "\n",
      "Categorical features: []\n",
      "Training data shape: (10964, 56)\n",
      "Train label shape: (10964,)\n",
      "Test data shape: (4700, 56)\n",
      "Test label shape: (4700,)\n"
     ]
    }
   ],
   "source": [
    "# Select equal sample of non-Pledge 1% organizations\n",
    "df_p1 = df_simple[df_simple['p1_tag']==1]\n",
    "df_notp1 = df_simple[df_simple['p1_tag']==0].sample(n=df_p1.shape[0], replace=False)\n",
    "df_model = pd.concat([df_p1, df_notp1]).reset_index(drop=True)\n",
    "\n",
    "# Create variable for each feature type: categorical and numerical\n",
    "numeric_features = df_model.select_dtypes(include=['int8', 'int16', 'int32', 'int64', 'float16', 'float32','float64']).drop(['p1_tag'], axis=1).columns\n",
    "categorical_features = df_model.select_dtypes(include=['object']).columns\n",
    "print('Numeric features:', numeric_features.to_list())\n",
    "print('\\nCategorical features:', categorical_features.to_list())\n",
    "\n",
    "X = df_model.drop('p1_tag', axis=1)\n",
    "y = df_model['p1_tag']\n",
    "y = preprocessing.LabelEncoder().fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "print('Training data shape:', X_train.shape)\n",
    "print('Train label shape:', y_train.shape)\n",
    "print('Test data shape:',  X_test.shape)\n",
    "print('Test label shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run through pipeline to determine best categorical feature encoder\n",
    "\n",
    "From: <a href='https://towardsdatascience.com/an-easier-way-to-encode-categorical-features-d840ff6b3900'>An Easier Way to Encode Categorical Features</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "Best parameter (CV score=0.729): {'classifier__C': 1000}\n",
      "Best score: 0.7376\n",
      "\n",
      "KNeighborsClassifier\n",
      "Best parameter (CV score=0.702): {'classifier__n_neighbors': 21}\n",
      "Best score: 0.6988\n",
      "\n",
      "BernoulliNB\n",
      "Best parameter (CV score=0.691): {'classifier__alpha': 10.0}\n",
      "Best score: 0.6892\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "classifier_list = []\n",
    "LRR = LogisticRegression()\n",
    "KNN = KNeighborsClassifier()\n",
    "BNB = BernoulliNB()\n",
    "classifier_list.append(('LogisticRegression', LRR, {'classifier__C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000]}))\n",
    "classifier_list.append(('KNeighborsClassifier', KNN, {'classifier__n_neighbors': np.arange(1,29,2)}))\n",
    "classifier_list.append(('BernoulliNB', BNB, {'classifier__alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]}))\n",
    "\n",
    "encoder_list = [ce.backward_difference.BackwardDifferenceEncoder]#, \n",
    "#                 ce.basen.BaseNEncoder,\n",
    "#                 ce.binary.BinaryEncoder,\n",
    "#                 ce.cat_boost.CatBoostEncoder,\n",
    "#                 ce.hashing.HashingEncoder,\n",
    "#                 ce.helmert.HelmertEncoder,\n",
    "#                 ce.james_stein.JamesSteinEncoder,\n",
    "#                 ce.one_hot.OneHotEncoder,\n",
    "#                 ce.leave_one_out.LeaveOneOutEncoder,\n",
    "#                 ce.m_estimate.MEstimateEncoder,\n",
    "#                 ce.ordinal.OrdinalEncoder,\n",
    "#                 ce.polynomial.PolynomialEncoder,\n",
    "#                 ce.sum_coding.SumEncoder,\n",
    "#                 ce.target_encoder.TargetEncoder,\n",
    "#                 ce.woe.WOEEncoder]\n",
    "\n",
    "for label, classifier, params in classifier_list:\n",
    "    results[label] = {}\n",
    "    print('{}'.format(label))\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[('scaler', MinMaxScaler())])\n",
    "#     categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median', fill_value='missing')),\n",
    "#                                               ('woe', encoder())])\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features)])\n",
    "                                                   #('cat', categorical_transformer, categorical_features)])\n",
    "    \n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', classifier)])\n",
    "    \n",
    "    if params != {}:\n",
    "        try:\n",
    "            search = GridSearchCV(pipe, params, n_jobs=-1)\n",
    "            search.fit(X_train, y_train)\n",
    "            print('Best parameter (CV score={:.3f}): {}'.format(search.best_score_, search.best_params_))\n",
    "            model = search.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            score = f1_score(y_test, y_pred)\n",
    "            print('Best score: {:.4f}\\n'.format(score))\n",
    "            results[label]['score'] = score\n",
    "            results[label]['best_params'] = search.best_params_\n",
    "        except:\n",
    "            print('Something went wrong w/ GridSearch or pipeline fitting.')\n",
    "    else:\n",
    "        try:\n",
    "            model = pipe.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            score = f1_score(y_test, y_pred)\n",
    "            print('Score: {:.4f}\\n'.format(score))\n",
    "            results[label]['score'] = score\n",
    "        except:\n",
    "            print('Something went wrong with pipeline fitting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our imputed, scaled, and modified model makes minimal improvement over the baseline, but considering we were ignoring missing values previously, this is still a good sign that we are headed in the right direction.\n",
    "\n",
    "Baseline scores:\n",
    "\n",
    "    Averaged Logistic Regression f1 Score: 0.7263\n",
    "    Averaged K-Nearest Neighbour f1 score: 0.6997\n",
    "    Averaged Naive Bayes f1 score: 0.6854"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS5sQetee7vc"
   },
   "source": [
    "***********************************************************\n",
    "# Final Model Pipeline\n",
    "\n",
    "By: Aditya Mengani, Ognjen Sosa, Sanjay Elangovan, Song Park, Sophia Skowronski\n",
    "\n",
    "### This pipeline:\n",
    "- Runs through a set of baseline and graph features using different iterative set up options\n",
    "- Performs an end to end pipeline implementation of classifers using RandomizedSearch \n",
    "- Displays calculated accuracies and tabulates the final scores\n",
    "***********************************************************\n",
    "### Model Context\n",
    "##### The models are run with the following combinations\n",
    "- Graph + Baseline\n",
    "- Baseline only\n",
    "- Graph + Baseline reduced\n",
    "- Baseline reduced only\n",
    "- Graph only\n",
    "***********************************************************\n",
    "### Classifiers\n",
    "- Logistic Regression\n",
    "- k_Nearest Neighbours\n",
    "- Bernoulli Naive Bayes\n",
    "- Decision Trees\n",
    "- Support Vector Machine(SVM)\n",
    "- Random Forest Classifier\n",
    "- XGBoost\n",
    "***********************************************************\n",
    "### Degree of Freedom\n",
    "- 4 and 5\n",
    "***********************************************************\n",
    "### Bootstrapping techniques\n",
    "- RandomizedSearchCV \n",
    "- Iterate and calculate average of scores for each setupType and degrees of freedom for each classifie\n",
    "***********************************************************\n",
    "### Component Analysis\n",
    "- PCA \n",
    "- Country and Industry Feature sets\n",
    "***********************************************************\n",
    "### Model relationships and set up types\n",
    "\n",
    "**Graph: `p1_tag` ~\t\t`k-core` + `min shortest path` + `shortest paths` + `degrees (in/out)` + `pagerank`**\n",
    "\n",
    "**Baseline: `p1_tag` ~\t\t`age` + `industry` + `employee count` + `country` + `rank` + `total funding`**\n",
    "\n",
    "**Baseline-R:\t`p1_tag` ~\t`age` + `industry` + `employee count` + `country`**\n",
    "\n",
    "**Graph + Baseline-Reduced: `p1_tag` ~ `Graph Features` + `Baseline Reduced Features`**\n",
    "\n",
    "**Graph + Baseline: `p1_tag` ~ `Graph Features` + `Baseline  Features`**\n",
    "***********************************************************\n",
    "### Input and Output File type and structure\n",
    "- Baseline (.csv) approx. 1 million observations\n",
    "    - Baseline File path: files/output/\n",
    "- Graph generated features (.csv)\n",
    "    - File path DF 4: files/output/Model_DF_D4/\n",
    "    - File path DF 5: files/output/Model_DF_D4/\n",
    "    - Folders 'B', 'G', 'GB', 'GBR', 'BR'\n",
    "- Output generated file: Generates 1 per `iteration`\n",
    "    - results_baseline_`iteration`.json\n",
    "    - File path files/output/\n",
    "\n",
    "**********************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvEpf7KR9ALa"
   },
   "source": [
    "### MODULE: BASELINE ONLY \n",
    "\n",
    "Generates the Baseline only features by reading the datasets for BL features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfO3_IhZ9ALa"
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "#####Steps##########\n",
    "## 1.Input dataframe (original baseline feature set ~ 1 million records)\n",
    "## 2.Extracted dataframe (UUIDS from files/output/Model_DF_D4 \n",
    "## or Model_DF_D5)\n",
    "## 3.Merge the two dataframes to get common list \n",
    "##################################################################\n",
    "\n",
    "def Baseline_Only(df,n_degrees, setup, iteration):\n",
    "    df = df.copy()\n",
    "    print(\"Original DF shape\",df.shape)\n",
    "    \n",
    "    # Have industry mapper for 'ind_1'...'ind_46' columns\n",
    "    industries = ['Software', 'Information Technology', 'Internet Services', 'Data and Analytics',\n",
    "                  'Sales and Marketing', 'Media and Entertainment', 'Commerce and Shopping', \n",
    "                  'Financial Services', 'Apps', 'Mobile', 'Science and Engineering', 'Hardware',\n",
    "                  'Health Care', 'Education', 'Artificial Intelligence', 'Professional Services', \n",
    "                  'Design', 'Community and Lifestyle', 'Real Estate', 'Advertising',\n",
    "                  'Transportation', 'Consumer Electronics', 'Lending and Investments',\n",
    "                  'Sports', 'Travel and Tourism', 'Food and Beverage',\n",
    "                  'Content and Publishing', 'Consumer Goods', 'Privacy and Security',\n",
    "                  'Video', 'Payments', 'Sustainability', 'Events', 'Manufacturing',\n",
    "                  'Clothing and Apparel', 'Administrative Services', 'Music and Audio',\n",
    "                  'Messaging and Telecommunications', 'Energy', 'Platforms', 'Gaming',\n",
    "                  'Government and Military', 'Biotechnology', 'Navigation and Mapping',\n",
    "                  'Agriculture and Farming', 'Natural Resources']\n",
    "    industry_map = {industry:'ind_'+str(idx+1) for idx,industry in enumerate(industries)}\n",
    "    \n",
    "    # reduce memory\n",
    "    df_simple = reduce_mem_usage(df)\n",
    "\n",
    "    print('\\nDataframe shape:', df_simple.shape)\n",
    "    del industries, industry_map\n",
    "        \n",
    "    # Extract baseline UUIDS part of Graph Network\n",
    "    list_Set_Up = ['BL_Only','G_Only','G+BL','G+BL_Red','BL_Red_Only']\n",
    "    folders = ['B', 'G', 'GB', 'GBR', 'BR']\n",
    "    save_map = dict(zip(list_Set_Up,folders))\n",
    "\n",
    "    # read the files based on the input setuptype/iteration/degrees\n",
    "    if n_degrees == 4:\n",
    "        df_bl = pd.read_csv('files/output/Model_DF_D4/{}/{}.csv'.format(save_map[setup], iteration),sep=',')\n",
    "        print(\"Original Model_DF_D2 shape\",df.shape)\n",
    "    # read the files based on the input setuptype/iteration/degrees   \n",
    "    elif n_degrees == 5:\n",
    "        df_bl = pd.read_csv('files/output/Model_DF_D5/{}/{}.csv'.format(save_map[setup], iteration),sep=',')\n",
    "        print(\"Original Model_DF_D4 shape\",df.shape)\n",
    "\n",
    "    # merge input dataframe with thre read baseline dataframe      \n",
    "    df_simple = pd.merge(df_bl.copy(),df_simple.copy(),how='inner',on='uuid') \n",
    "    \n",
    "    return df_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I87MV1199ALb"
   },
   "source": [
    "### MODULE: BASELINE REDUCED \n",
    "\n",
    "Eliminates FEATURES: RANK and total_funding_usd (as there are not much improvements with these feature sets as part of preliminary eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CERZmDOs9ALb"
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "#####Steps##########\n",
    "## 1.Input dataframe (original baseline feature set ~ 1 million records)\n",
    "## 2.Extracted dataframe (UUIDS from files/output/Model_DF_D4 \n",
    "## or Model_DF_D5)\n",
    "## 3. Eliminate the features not required (Rank and total_funding_usd) \n",
    "## 4.Merge the two dataframes to get common list \n",
    "##################################################################\n",
    "\n",
    "def Baseline_Reduced(df,n_degrees, setup, iteration):\n",
    "    df = df.copy()\n",
    "    print(\"Original DF shape\",df.shape)\n",
    "    \n",
    "    # Have industry mapper for 'ind_1'...'ind_46' columns\n",
    "    industries = ['Software', 'Information Technology', 'Internet Services', 'Data and Analytics',\n",
    "                  'Sales and Marketing', 'Media and Entertainment', 'Commerce and Shopping', \n",
    "                  'Financial Services', 'Apps', 'Mobile', 'Science and Engineering', 'Hardware',\n",
    "                  'Health Care', 'Education', 'Artificial Intelligence', 'Professional Services', \n",
    "                  'Design', 'Community and Lifestyle', 'Real Estate', 'Advertising',\n",
    "                  'Transportation', 'Consumer Electronics', 'Lending and Investments',\n",
    "                  'Sports', 'Travel and Tourism', 'Food and Beverage',\n",
    "                  'Content and Publishing', 'Consumer Goods', 'Privacy and Security',\n",
    "                  'Video', 'Payments', 'Sustainability', 'Events', 'Manufacturing',\n",
    "                  'Clothing and Apparel', 'Administrative Services', 'Music and Audio',\n",
    "                  'Messaging and Telecommunications', 'Energy', 'Platforms', 'Gaming',\n",
    "                  'Government and Military', 'Biotechnology', 'Navigation and Mapping',\n",
    "                  'Agriculture and Farming', 'Natural Resources']\n",
    "    industry_map = {industry:'ind_'+str(idx+1) for idx,industry in enumerate(industries)}\n",
    "    \n",
    "\n",
    "    # Reduced baseline doesnt have these two columns\n",
    "    df_simple = df.drop(['rank','total_funding_usd'], axis=1)\n",
    "    df_simple = reduce_mem_usage(df_simple)\n",
    "    print('\\nDataframe shape:', df_simple.shape)\n",
    "    \n",
    "    # Extract baseline UUIDS part of Graph Network\n",
    "    list_Set_Up = ['BL_Only','G_Only','G+BL','G+BL_Red','BL_Red_Only']\n",
    "    folders = ['B', 'G', 'GB', 'GBR', 'BR']\n",
    "    save_map = dict(zip(list_Set_Up,folders))\n",
    "\n",
    "    # read the files based on the input setuptype/iteration/degrees\n",
    "    if n_degrees == 4:\n",
    "        df_bl = pd.read_csv('files/output/Model_DF_D4/{}/{}.csv'.format(save_map[setup], iteration),sep=',')\n",
    "        print(\"Original Model_DF_D2 shape\",df.shape)\n",
    "    # read the files based on the input setuptype/iteration/degrees\n",
    "    elif n_degrees == 5:\n",
    "        df_bl = pd.read_csv('files/output/Model_DF_D5/{}/{}.csv'.format(save_map[setup], iteration),sep=',')\n",
    "        print(\"Original Model_DF_D4 shape\",df.shape)\n",
    "    \n",
    "    # merge input dataframe with thre read baseline dataframe \n",
    "    df_simple = pd.merge(df_bl.copy(),df_simple.copy(),how='inner',on='uuid')   \n",
    "    \n",
    "    del industries, industry_map\n",
    "    return df_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCGQIcD29ALc"
   },
   "source": [
    "### MODULE : GRAPH ONLY\n",
    "\n",
    "Generates the Graph only features from extracting the data from graph features to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SlRFTN29ALc"
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "#####Steps##########\n",
    "## 1. Input the baseline dataframe\n",
    "## 2.Extract graph dataframe (From files/output/Model_DF_D4 \n",
    "## or Model_DF_D5 related to the graph generated features) \n",
    "## 3.Impute graph features fetching infinite values\n",
    "## 4.Merge the two dataframes to get common list \n",
    "##################################################################\n",
    "def Graph_Only_SS(df,n_degrees, setup, iteration):\n",
    "    df = df.copy()\n",
    "\n",
    "    #select uuid and p1_tag from baseline dataframe\n",
    "    df = df[['uuid','p1_tag']]\n",
    "    print(\"Original DF shape\",df.shape)\n",
    "\n",
    "    # read the files based on the input df setuptype/iteration/degress\n",
    "    list_Set_Up = ['BL_Only','G_Only','G+BL','G+BL_Red','BL_Red_Only']\n",
    "    folders = ['B', 'G', 'GB', 'GBR', 'BR']\n",
    "    save_map = dict(zip(list_Set_Up,folders))\n",
    "    \n",
    "    # read the files based on the input setuptype/iteration/degrees\n",
    "    if n_degrees == 4:\n",
    "        df_gr = pd.read_csv('files/output/Model_DF_D4/{}/{}.csv'.format(save_map[setup], iteration),sep=',')\n",
    "        print(\"Original Model_DF_D2 shape\",df.shape)\n",
    "    # read the files based on the input setuptype/iteration/degrees\n",
    "    elif n_degrees == 5:\n",
    "        df_gr = pd.read_csv('files/output/Model_DF_D5/{}/{}.csv'.format(save_map[setup], iteration),sep=',')\n",
    "        print(\"Original Model_DF_D4 shape\",df.shape)\n",
    "    \n",
    "    # merge input dataframe with thre read baseline dataframe \n",
    "    df_gr = pd.merge(df_gr.copy(),df.copy(),how='inner',on='uuid')\n",
    "    print(\"Original DF_GR shape after merge\",df_gr.shape)\n",
    "\n",
    "    # reduce memory\n",
    "    df_gr = reduce_mem_usage(df_gr) \n",
    "\n",
    "    # impute infinite spath values with 1000\n",
    "    df_gr['w_spath_top_3_0'][df_gr['w_spath_top_3_0']==1e30] = 1000\n",
    "    df_gr['w_spath_top_3_1'][df_gr['w_spath_top_3_1']==1e30] = 1000\n",
    "    df_gr['w_spath_top_3_3'][df_gr['w_spath_top_3_3']==1e30] = 1000\n",
    "    df_gr['w_spath_top_3_4'][df_gr['w_spath_top_3_4']==1e30] = 1000\n",
    "    df_gr['w_spath_top_min_3'][df_gr['w_spath_top_min_3']==1e30] = 1000\n",
    "    \n",
    "    # impute any na values\n",
    "    df_gr = df_gr.fillna(0)\n",
    "       \n",
    "    del df\n",
    "    return df_gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwCEkzLB9ALd"
   },
   "source": [
    "### MODULE: GENERATE TRAIN TEST SPLIT\n",
    "\n",
    "Generates train test split for the input dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UaDSZ2aje7ve"
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "#####Steps##########\n",
    "## 1.Input the dataframe to be split\n",
    "## 2.Randomly sample the data to pick equal count of non-P1 companies\n",
    "## 3.Split the data into train/test 80:20\n",
    "##################################################################\n",
    "## Select equal sample of non-Pledge 1% organizations\n",
    "def gen_Train_Test_Split(df_simple):\n",
    "\n",
    "    # get all the p1 companies\n",
    "    df_p1 = df_simple[df_simple['p1_tag']==1]\n",
    "    print(df_p1.shape)\n",
    "\n",
    "    # sample randomly all the non-p1 companies for equal sample size of p1\n",
    "    df_notp1 = df_simple[df_simple['p1_tag']==0].sample(n=df_p1.shape[0], replace=True)\n",
    "\n",
    "    # concat p1 and non-p1 companies\n",
    "    df_model = pd.concat([df_p1, df_notp1]).reset_index(drop=True)\n",
    "\n",
    "    # reduce memory\n",
    "    df_model = reduce_mem_usage(df_model)\n",
    "\n",
    "    # Create variable for each feature type: categorical and numerical\n",
    "    numeric_features = df_model.select_dtypes(include=['uint8','int8', 'int16', 'int32', 'int64', 'float16', 'float32','float64']).drop(['p1_tag'], axis=1).columns\n",
    "    categorical_features = df_model.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Select all labels except the output\n",
    "    X = df_model.drop('p1_tag', axis=1)\n",
    "\n",
    "    # select precdictor label\n",
    "    y = df_model['p1_tag']\n",
    "    y = preprocessing.LabelEncoder().fit_transform(y)\n",
    "\n",
    "    # create a train/test split for 80/20 in the sample data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=99)\n",
    "    print('Training data shape:', X_train.shape)\n",
    "    print('Train label shape:', y_train.shape)\n",
    "    print('Test data shape:',  X_test.shape)\n",
    "    print('Test label shape:', y_test.shape)\n",
    "\n",
    "    # reset indexes for train and test\n",
    "    X_train= X_train.reset_index(drop=True)\n",
    "    X_test= X_test.reset_index(drop=True)\n",
    "    return X_train,X_test,X,y,y_train,y_test,numeric_features,categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "numClLyL9ALd"
   },
   "source": [
    "### MODULE : PERFORM PCA COUNTRY\n",
    "\n",
    "Perform PCA on country code feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hfb6xuWChK3Z"
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "#####Steps##########\n",
    "## 1.Input the train and test datasets\n",
    "## 2.Perform PCA analysis on country attributes\n",
    "## 3.Plot a graph for Fraction of total variance vs. number of principal components\n",
    "## 4.Run PCA, transform the data into reduced components\n",
    "##################################################################\n",
    "\n",
    "# Perform PCA of country dataset\n",
    "def PCA_Country(X_train,X_test):\n",
    "\n",
    "    # Perform PCA of country dataset\n",
    "    # read all the attributes that belongs to the country features\n",
    "    country_train = X_train.filter(regex='^country',axis=1).fillna(0)\n",
    "    country_test = X_test.filter(regex='^country',axis=1).fillna(0)\n",
    "#     # For each value of k, use PCA to project the data feature sets to k principle components\n",
    "#     matrix = [['k', 'total variance']] # For display\n",
    "#     k_values = list(range(1,113)) # To loop through, there are 112 country codes\n",
    "#     # For each value of k, use PCA to project the data feature sets to k principle components\n",
    "#     for k in k_values:\n",
    "#         pca = PCA(n_components=k, whiten=True,random_state=random.seed(1234))\n",
    "#         pca.fit(country_train)\n",
    "#         matrix.append([k, round(pca.explained_variance_ratio_.sum(),4)])\n",
    "#     # Print results\n",
    "#     print('Fraction of the total variance in the training data explained by the first k principal components:\\n')\n",
    "#     s = [[str(e) for e in row] for row in matrix]\n",
    "#     lens = [max(map(len, col)) for col in zip(*s)]\n",
    "#     fmt = '\\t'.join('{{:{}}}'.format(x) for x in lens)\n",
    "#     table = [fmt.format(*row) for row in s]\n",
    "#     print('\\n'.join(table))\n",
    "#     print()\n",
    "#     # Plots\n",
    "#     _, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,5))\n",
    "#     # Plotting lineplot of fraction of total variance vs. number of principal components\n",
    "#     # For all possible numbers of principal components\n",
    "#     ax.plot(np.cumsum(PCA().fit(country_train).explained_variance_ratio_))\n",
    "#     # Labels\n",
    "#     ax.set_title('Fraction of total variance vs. number of principal components')\n",
    "#     ax.set_xlabel('k = number of components')\n",
    "#     ax.set_ylabel('Cumulative explained variance')\n",
    "#     # Display\n",
    "#     plt.show()\n",
    "    \n",
    "    # create PCA features for train and test set\n",
    "    #print(\"country train\",list(country_train.columns))\n",
    "\n",
    "    # The above commented plot identifies PCA n_components as the best fit of variance\n",
    "    n_components = 15\n",
    "\n",
    "    # Run pca and create new features based on the n_components = 15\n",
    "    pca = PCA(n_components=n_components,whiten=True,random_state=random.seed(1234))  \n",
    "    \n",
    "    # create a train pca dataset\n",
    "    pca_train = pca.fit_transform(country_train)\n",
    "    # create a test pca dataset\n",
    "    pca_test = pca.transform(country_test)\n",
    "    \n",
    "    # create dataframes from numpy pca features\n",
    "    df_cty_train = pd.DataFrame(pca_train,columns=['cntry_pca_'+ str(x) for x in range(n_components)])\n",
    "    df_cty_test = pd.DataFrame(pca_test,columns=['cntry_pca_'+ str(x) for x in range(n_components)])\n",
    "    \n",
    "    # drop country prefix columns\n",
    "    X_train = X_train.drop(list(X_train.filter(regex='^country_',axis=1).columns), axis=1)\n",
    "    X_test = X_test.drop(list(X_test.filter(regex='^country_',axis=1).columns), axis=1)\n",
    "    \n",
    "    # concat with train dataset\n",
    "    X_train = pd.concat([X_train, df_cty_train],axis = 1)\n",
    "    X_test = pd.concat([X_test, df_cty_test],axis = 1)\n",
    "    \n",
    "    # delete dataframes\n",
    "    del df_cty_train,df_cty_test,country_train,country_test\n",
    "    return X_train,X_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyiZwGk09ALe"
   },
   "source": [
    "### MODULE : PERFORM PCA INDUSTRY \n",
    "\n",
    "Perform PCA on Industry feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNCYcDu9gj_z"
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "#####Steps##########\n",
    "## 1.Input the train and test datasets\n",
    "## 2.Perform PCA analysis on industry attributes\n",
    "## 3.Plot a graph for Fraction of total variance vs. number of principal components\n",
    "## 4.Run PCA, transform the data into reduced components\n",
    "##################################################################\n",
    "\n",
    "# Perform PCA of country dataset\n",
    "def PCA_Industry(X_train,X_test):\n",
    "    \n",
    "    # Perform PCA of industry dataset\n",
    "    # Perform PCA of country dataset\n",
    "    # read all the attributes that belongs to the country features\n",
    "    industry_train = X_train.filter(regex='^ind_',axis=1).fillna(0)\n",
    "    industry_test = X_test.filter(regex='^ind_',axis=1).fillna(0)\n",
    "#     # For each value of k, use PCA to project the data feature sets to k principle components\n",
    "#     matrix = [['k', 'total variance']] # For display\n",
    "#     k_values = list(range(1,47)) # To loop through, there are 46 industries\n",
    "#     # For each value of k, use PCA to project the data feature sets to k principle components\n",
    "#     for k in k_values:\n",
    "#         pca = PCA(n_components=k, whiten=True,random_state=random.seed(1234))\n",
    "#         pca.fit(industry_train)\n",
    "#         matrix.append([k, round(pca.explained_variance_ratio_.sum(),4)])\n",
    "#     # Print results\n",
    "#     print('Fraction of the total variance in the training data explained by the first k principal components:\\n')\n",
    "#     s = [[str(e) for e in row] for row in matrix]\n",
    "#     lens = [max(map(len, col)) for col in zip(*s)]\n",
    "#     fmt = '\\t'.join('{{:{}}}'.format(x) for x in lens)\n",
    "#     table = [fmt.format(*row) for row in s]\n",
    "#     print('\\n'.join(table))\n",
    "#     print()\n",
    "#     # Plots\n",
    "#     _, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,5))\n",
    "#     # Plotting lineplot of fraction of total variance vs. number of principal components\n",
    "#     # For all possible numbers of principal components\n",
    "#     ax.plot(np.cumsum(PCA().fit(industry_train).explained_variance_ratio_))\n",
    "#     # Labels\n",
    "#     ax.set_title('Fraction of total variance vs. number of principal components')\n",
    "#     ax.set_xlabel('k = number of components')\n",
    "#     ax.set_ylabel('Cumulative explained variance')\n",
    "#     # Display\n",
    "#     plt.show()\n",
    "    \n",
    "    # create PCA features for train and test set\n",
    "\n",
    "    # The above commented plot identifies PCA n_components as the best fit of variance\n",
    "    n_components=10\n",
    "    \n",
    "    # Run pca and create new features based on the n_components = 15\n",
    "    pca = PCA(n_components=n_components, whiten=True, random_state=random.seed(1234)) \n",
    "    \n",
    "    # create a train pca dataset\n",
    "    pca_train = pca.fit_transform(industry_train)\n",
    "    \n",
    "    # create a test pca dataset\n",
    "    pca_test = pca.transform(industry_test)\n",
    "    \n",
    "    # create dataframes from numpy pca features\n",
    "    df_ind_train = pd.DataFrame(pca_train,columns=['ind_pca'+ str(x) for x in range(n_components)])\n",
    "    df_ind_test = pd.DataFrame(pca_test,columns=['ind_pca'+ str(x) for x in range(n_components)])\n",
    "    \n",
    "    # drop country prefix columns\n",
    "    X_train = X_train.drop(list(X_train.filter(regex='^ind_',axis=1).columns), axis=1)\n",
    "    X_test = X_test.drop(list(X_test.filter(regex='^ind_',axis=1).columns), axis=1)\n",
    "    \n",
    "    # concat with train dataset\n",
    "    X_train = pd.concat([X_train, df_ind_train],axis = 1)\n",
    "    X_test = pd.concat([X_test, df_ind_test],axis = 1)\n",
    "    \n",
    "    # delete dataframes\n",
    "    del df_ind_train,df_ind_test,industry_train,industry_test\n",
    "\n",
    "    return X_train,X_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xIeI6pK9ALf"
   },
   "source": [
    "### MODULE: VIZUALIZE COUNTRY & INDUSTRY PCA\n",
    "\n",
    "Visualize Country & Industry PCA spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UYBtgwajQZ8L"
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "#####Steps##########\n",
    "## 1.Input the data and labels\n",
    "## 2.Plot a graph for PCA distribution of Industry and Country\n",
    "##################################################################\n",
    "\n",
    "# create graphs for PCA analysis for country and industry features\n",
    "def Visualize_Country_Ind_PCA(X,y):\n",
    "    print(\"None\")\n",
    "#     Country_df = X.filter(regex='^country',axis=1).fillna(0)\n",
    "#     pca_new_Country = PCA(n_components=10,random_state=random.seed(1234))  \n",
    "#     Country_df_PCA = pca_new_Country.fit_transform(Country_df)\n",
    "\n",
    "#     Industry_df = X.filter(regex='^ind_',axis=1).fillna(0)\n",
    "#     pca_new_Industry_df = PCA(n_components=30,random_state=random.seed(1234))  \n",
    "#     Industry_df_PCA = pca_new_Industry_df.fit_transform(Industry_df)\n",
    "\n",
    "#     # The PCA model\n",
    "#     fig, axes = plt.subplots(1,2,figsize=(15,15))\n",
    "#     colors = ['r','g']\n",
    "#     fig.suptitle('PCA Analysis for Country and Industry', fontsize=30)\n",
    "#     targets = [1,0]\n",
    "#     for target, color in zip(targets,colors):\n",
    "#       indexes = np.where(y == target)\n",
    "#       axes[0].scatter(Country_df_PCA[indexes][:,0], Country_df_PCA[indexes][:,1],color=color)\n",
    "#       axes[0].set_xlabel('PC1')\n",
    "#       axes[0].set_ylabel('PC2')\n",
    "#       axes[0].set_title('PCA-Country')\n",
    "#       axes[1].scatter(Industry_df_PCA[indexes][:,0], Industry_df_PCA[indexes][:,1], color=color)\n",
    "#       axes[1].set_xlabel('PC1')\n",
    "#       axes[1].set_ylabel('PC2')\n",
    "#       axes[1].set_title('PCA-Industry')\n",
    "#     plt.axis('tight')\n",
    "\n",
    "#     out_labels = ['p1','non-p1']\n",
    "#     plt.legend(out_labels,prop={'size':10},loc='upper right',title='Legend of plot')\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yp1GKxz39ALf"
   },
   "source": [
    "### MODULE: RUN CLASSIFIER\n",
    "\n",
    " Uncomment the classifier that you need to run and comment the ones that you are not running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1607615734344,
     "user": {
      "displayName": "Aditya Mengani",
      "photoUrl": "",
      "userId": "10786983343714336915"
     },
     "user_tz": 360
    },
    "id": "BmnuF44ge7ve"
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "#####Steps##########\n",
    "## 1.Input the feature sets, train, test data and \n",
    "##    labels,categorical and numerical features\n",
    "## 2.Define all the models to be evaluated\n",
    "## 3.Create a pipeline\n",
    "## 4. In pipeline perform:\n",
    "            # - Encoding\n",
    "            # - Scaling\n",
    "            # - Simple Imputing\n",
    "            # - GridSearch/RandomizedSearch\n",
    "            # - Fit train data/Predict test data\n",
    "            # - Evaluate accuracy score\n",
    "            # - Append the scores to results\n",
    "##################################################################\n",
    "\n",
    "def Run_Classifier(X_train,X_test,y_train,y_test,numeric_features,categorical_features,n_deg,Type):\n",
    "    # create a dict of results and append degrees and set up type\n",
    "    results = OrderedDict()\n",
    "    results['n_deg'] = n_deg\n",
    "    results['Model_Type'] = Type\n",
    "    #results['Column_Name'] = col_graph\n",
    "\n",
    "    # create classifier list\n",
    "    classifier_list = []\n",
    "\n",
    "    # define classification models\n",
    "    LRR = LogisticRegression(max_iter=10000, tol=0.1)\n",
    "    KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "#     BNB = BernoulliNB()\n",
    "#     GNB = GaussianNB()\n",
    "#     SVM = svm.SVC()\n",
    "#     DCT = DecisionTreeClassifier()\n",
    "#     XGB = xgb.XGBRegressor() #tree_method='gpu_hist', gpu_id=0\n",
    "#     RMF = RandomForestClassifier()\n",
    "\n",
    "    # add classifier parmeters to classifier list\n",
    "    classifier_list.append(('LRR', LRR, {'classifier__C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000],\\\n",
    "                                        'classifier__random_state': [random.seed(1234)]}))\n",
    "    classifier_list.append(('KNN', KNN, {}))\n",
    "#     classifier_list.append(('BNB', BNB, {'classifier__alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]}))\n",
    "#     classifier_list.append(('GNB', GNB, {'classifier__var_smoothing': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]}))\n",
    "#     classifier_list.append(('DCT', DCT, {'classifier__max_depth':np.arange(1, 21),\n",
    "#                                         'classifier__min_samples_leaf':[1, 5, 10, 20, 50, 100],\n",
    "#                                         'classifier__random_state' : [random.seed(1234)]}))\n",
    "#     classifier_list.append(('XGB', XGB, {'classifier__random_state' : [random.seed(1234)]}))\n",
    "#     classifier_list.append(('RMF', RMF, {'classifier__random_state' : [random.seed(1234)]}))\n",
    "#     classifier_list.append(('SVM', SVM, {'classifier__random_state' : [random.seed(1234)]}))\n",
    "\n",
    "    # define encoder lsit\n",
    "    encoder_list = [ce.one_hot.OneHotEncoder]\n",
    "\n",
    "    # define scaling list\n",
    "    scaler_list = [StandardScaler()]\n",
    "\n",
    "    # for each lablel/classifier/parmeter in classifier list\n",
    "    # iterate over gridsearch and run the classifers/encoders/scalers\n",
    "    # store the result in the dictionary\n",
    "    for label, classifier, params in classifier_list:\n",
    "        results[label] = {}\n",
    "        #for each encoder\n",
    "        for encoder in encoder_list:\n",
    "            # for each feature scaler\n",
    "            for feature_scaler in scaler_list:\n",
    "                # for each results label \n",
    "                results[label][f'{encoder.__name__} with {feature_scaler}'] = {}\n",
    "                print('{} with {} and {}'.format(label,encoder.__name__,feature_scaler))\n",
    "                # define numerical transformer using standard scaler\n",
    "                numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),('scaler', StandardScaler())])\n",
    "                # define categorical imputer steps\n",
    "                categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                                                          ('woe', encoder())])\n",
    "                # define preprocessor for column transformations\n",
    "                preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features),\n",
    "                                                               ('cat', categorical_transformer, categorical_features)])\n",
    "                # run pipleine for preprocessor/scaler/classifier\n",
    "                pipe = Pipeline(steps=[#('preprocessor', preprocessor),\n",
    "                                       ('scaler', feature_scaler),\n",
    "                                       ('classifier', classifier)])\n",
    "                # if params are available\n",
    "                if params != {}:\n",
    "                    # perform randomizedsearchCV\n",
    "                    search = RandomizedSearchCV(pipe, params, n_jobs=-1)\n",
    "                    # fit the randomizedSearchCV object with train features and labels\n",
    "                    search.fit(X_train, y_train)\n",
    "                    # display the best parameter\n",
    "                    print('Best parameter (CV score={:.3f}): {}'.format(search.best_score_, search.best_params_))\n",
    "                    # create the model from the best fit values of train features and label\n",
    "                    model = search.fit(X_train, y_train)\n",
    "                    # predict the labels for test features\n",
    "                    y_pred = model.predict(X_test)\n",
    "\n",
    "                    # if the classifier is xgboost round the values to convert to binary\n",
    "                    # standard process to convert it for logistic regression\n",
    "                    if label == 'XGB':\n",
    "                        y_pred = [round(value) for value in y_pred]\n",
    "\n",
    "                    # calculate the score and display best score and populate results\n",
    "                    score = f1_score(y_test, y_pred,average='weighted')\n",
    "                    print('Best score: {:.4f}\\n'.format(score))\n",
    "                    results[label][f'{encoder.__name__} with {feature_scaler}']['score'] = score\n",
    "                    try:\n",
    "                        results[label][f'{encoder.__name__} with {feature_scaler}']['best_params'] = search.best_params_\n",
    "                    except:\n",
    "                        print('Something went wrong w/ GridSearch or pipeline fitting.')\n",
    "                else:\n",
    "                    # if the does not have any parameters \n",
    "                    try:\n",
    "                        # fit the train labels and features to the pipeline\n",
    "                        model = pipe.fit(X_train, y_train)\n",
    "                        # predict the model with the test features\n",
    "                        y_pred = model.predict(X_test)\n",
    "                        # if the classifier is xgboost round the values to convert to binary\n",
    "                        # standard process to convert it for logistic regression\n",
    "                        if label == 'XGB':\n",
    "                            y_pred = [round(value) for value in y_pred]\n",
    "                        \n",
    "                        # calculate the score and display best score and populate results\n",
    "                        score = f1_score(y_test, y_pred,average='weighted')\n",
    "                        print('Score: {:.4f}\\n'.format(score))\n",
    "                        results[label][f'{encoder.__name__} with {feature_scaler}']['score'] = score\n",
    "                    except:\n",
    "                        print('Something went wrong with pipeline fitting')\n",
    "    return results    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ls62dYWg9ALg"
   },
   "source": [
    "### MODULE: WRITE OUTPUT\n",
    "\n",
    "Generate output json file for the model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAdxD11h9ALh"
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "#####Steps##########\n",
    "## 1.Input the result list generated\n",
    "## 2.Run this through json encoder to convert the file to json\n",
    "## 3.Save the results into a json file\n",
    "##################################################################\n",
    "def Write_Output(out_list,iteration):\n",
    "    # encode to encode int/float and array types and write the output json\n",
    "    class NpEncoder(json.JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            else:\n",
    "                return super(NpEncoder, self).default(obj)\n",
    "\n",
    "    # File is saved under Files directory. /content would be the baseline folder\n",
    "    # You can click on folder icon on left side of the directory structure to\n",
    "    # see the created file\n",
    "    \n",
    "    with open(f'files/output/results_baseline_ITER_{iteration}.json', 'w') as fp:\n",
    "        json.dump(out_list, fp, sort_keys=False, indent=4, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Geosjzw9ALh"
   },
   "source": [
    "### MODULE : REDUCE MEMORY USAGE\n",
    "\n",
    "Module to reduce memory usage for dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztlFmgJr9ALh"
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "#####Steps##########\n",
    "## 1.Input dataframe from original module\n",
    "## 2.Identify min and max ranges of sizes for each column\n",
    "## 3.Apply typecasting to reduce the memory usage \n",
    "##################################################################\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100*(start_mem-end_mem)/start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8cvI9WA9ALi"
   },
   "source": [
    "### MODULE: CALCULATE AVERAGE SCORE\n",
    "\n",
    "Calculate the average score of all the generated files for a particular model type/model set up combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ljhoZ3h9ALi"
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "#####Steps##########\n",
    "## 1.Input the generated results file\n",
    "## 2.Iterate it over each model/setup/degree type\n",
    "## 3.Caclulate the average and display it\n",
    "##################################################################\n",
    "\n",
    "# Open the generated results file\n",
    "def calculate_avg(iterations):\n",
    "    # Enter the model names #'KNN':0,'BNB':0,'GNB':0,'DCT':0,'XGB':0,'RMF':0,'SVM':0\n",
    "    test_accuracies = {'LRR':0}\n",
    "    data_cnt = {'LRR':0}\n",
    "    \n",
    "    for i in range(iterations):\n",
    "         with open(f'files/output/results_baseline_ITER_{i}.json') as g:\n",
    "                #json.dump(out_list, fp, sort_keys=False, indent=4, cls=NpEncoder)\n",
    "                data = json.load(g)\n",
    "                for i in list(test_accuracies.keys()):\n",
    "                    for j in data:\n",
    "                        test_accuracies[i] = test_accuracies[i] + (j['result'][0][i]['OneHotEncoder with StandardScaler()']['score'])\n",
    "                        data_cnt[i] = data_cnt[i] + len(data)\n",
    "    \n",
    "    for i in test_accuracies:\n",
    "        test_accuracies[i] = round(test_accuracies[i]/data_cnt[i],2)\n",
    "\n",
    "  \n",
    "\n",
    "    print(\"\\nAveraged accuracies: \",test_accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YRSMJkA9ALi"
   },
   "source": [
    "### MODULE: GENERATE TABLE\n",
    "\n",
    "This module generates a dataframe for model/set up/degree combination and displays it as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjeBS1Bk9ALi"
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "#####Steps##########\n",
    "## 1.Input the generated results files and read them to a dataframe\n",
    "## 2.Iterate it over each model/setup/degree type\n",
    "## 3.Caclulate the average and display it as a table\n",
    "##################################################################\n",
    "\n",
    "#List of json files\n",
    "\n",
    "def generate_table():\n",
    "    file_list = ['results_baseline_ITER_0.json','results_baseline_ITER_1.json','results_baseline_ITER_2.json','results_baseline_ITER_3.json',\n",
    "                 'results_baseline_ITER_4.json','results_baseline_ITER_5.json','results_baseline_ITER_6.json','results_baseline_ITER_7.json',\n",
    "                 'results_baseline_ITER_8.json','results_baseline_ITER_9.json']\n",
    "\n",
    "    #Function for flattening nested json\n",
    "    def flatten_json(nested_json, exclude=['']):\n",
    "        out = {}\n",
    "        def flatten(x, name='', exclude=exclude):\n",
    "            if type(x) is dict:\n",
    "                for a in x:\n",
    "                    if a not in exclude: flatten(x[a], name + a + '_')\n",
    "            elif type(x) is list:\n",
    "                i = 0\n",
    "                for a in x:\n",
    "                    flatten(a, name + str(i) + '_')\n",
    "                    i += 1\n",
    "            else:\n",
    "                out[name[:-1]] = x\n",
    "        flatten(nested_json)\n",
    "        return out\n",
    "    \n",
    "    #Loop through list of json files, convert to df and append\n",
    "    df = pd.DataFrame([])\n",
    "    for file in file_list:\n",
    "        with open(f'files/output/{file}') as train_file:\n",
    "            dict_train = json.load(train_file)\n",
    "        df = df.append(pd.DataFrame([flatten_json(x) for x in dict_train[0]['result']]))\n",
    "\n",
    "    df = df.groupby(['n_deg','Model_Type']).mean().round(3).reset_index()\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbmDGBoR9ALi"
   },
   "source": [
    "### MODULE: MAIN \n",
    "- Run 10 iterations,for each set up and each degree type\n",
    "- Capture the results in json\n",
    "- Calculate the average across all scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DQF3go49ALj",
    "outputId": "447474a7-696e-440d-e023-b80a9fae2738",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ITERATION:0:DEGREE:4:SETUP:BL_Only BEGIN...\n",
      "\n",
      "ITERATION:0:DEGREE:4:BASELINE_ONLY SET UP:START...\n",
      "Original DF shape (1010412, 264)\n",
      "Mem. usage decreased to 313.17 Mb (84.6% reduction)\n",
      "\n",
      "Dataframe shape: (1010412, 264)\n",
      "Original Model_DF_D2 shape (1010412, 264)\n",
      "(3868, 263)\n",
      "Mem. usage decreased to  2.34 Mb (0.0% reduction)\n",
      "Training data shape: (6188, 262)\n",
      "Train label shape: (6188,)\n",
      "Test data shape: (1548, 262)\n",
      "Test label shape: (1548,)\n",
      "Final train dataset shape (6188, 29)\n",
      "\n",
      "Final test dataset shape (1548, 29)\n",
      "LRR with OneHotEncoder and StandardScaler()\n",
      "Best parameter (CV score=0.707): {'classifier__random_state': None, 'classifier__C': 1.0}\n",
      "Best score: 0.7079\n",
      "\n",
      "KNN with OneHotEncoder and StandardScaler()\n",
      "Score: 0.6990\n",
      "\n",
      "\n",
      "ITERATION:0:DEGREE:4:BASELINE_ONLY SET UP:END\n",
      "\n",
      "ITERATION:0:DEGREE:4:END\n",
      "\n",
      "ITERATION:1:DEGREE:4:SETUP:BL_Only BEGIN...\n",
      "\n",
      "ITERATION:1:DEGREE:4:BASELINE_ONLY SET UP:START...\n",
      "Original DF shape (1010412, 264)\n",
      "Mem. usage decreased to 313.17 Mb (84.6% reduction)\n",
      "\n",
      "Dataframe shape: (1010412, 264)\n",
      "Original Model_DF_D2 shape (1010412, 264)\n",
      "(3866, 263)\n",
      "Mem. usage decreased to  2.34 Mb (0.0% reduction)\n",
      "Training data shape: (6185, 262)\n",
      "Train label shape: (6185,)\n",
      "Test data shape: (1547, 262)\n",
      "Test label shape: (1547,)\n",
      "Final train dataset shape (6185, 29)\n",
      "\n",
      "Final test dataset shape (1547, 29)\n",
      "LRR with OneHotEncoder and StandardScaler()\n",
      "Best parameter (CV score=0.720): {'classifier__random_state': None, 'classifier__C': 1.0}\n",
      "Best score: 0.7259\n",
      "\n",
      "KNN with OneHotEncoder and StandardScaler()\n",
      "Score: 0.6956\n",
      "\n",
      "\n",
      "ITERATION:1:DEGREE:4:BASELINE_ONLY SET UP:END\n",
      "\n",
      "ITERATION:1:DEGREE:4:END\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_deg</th>\n",
       "      <th>Model_Type</th>\n",
       "      <th>LRR_OneHotEncoder with StandardScaler()_score</th>\n",
       "      <th>LRR_OneHotEncoder with StandardScaler()_best_params_classifier__C</th>\n",
       "      <th>KNN_OneHotEncoder with StandardScaler()_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>BL_Only</td>\n",
       "      <td>0.709</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>BL_Red_Only</td>\n",
       "      <td>0.710</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>G+BL</td>\n",
       "      <td>0.794</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>G+BL_Red</td>\n",
       "      <td>0.789</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>G_Only</td>\n",
       "      <td>0.684</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>BL_Only</td>\n",
       "      <td>0.710</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>BL_Red_Only</td>\n",
       "      <td>0.718</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>G+BL</td>\n",
       "      <td>0.813</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>G+BL_Red</td>\n",
       "      <td>0.803</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>G_Only</td>\n",
       "      <td>0.690</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed all runs!....\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "#####Steps##########\n",
    "## 1.Define setup Types/ Degree of freedom/ Iterations\n",
    "## 2.Iterate over each combination\n",
    "## 3.For each combination of degree and set up, call the \n",
    "## modules defined earlier\n",
    "## 4.Save results and generate a table of accuracies \n",
    "##################################################################\n",
    "\n",
    "## import modules\n",
    "'''Data analysis'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "#import itertoolss\n",
    "import statistics\n",
    "from collections import OrderedDict \n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "'''Plotting'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "'''Stat'''\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import chi2_contingency\n",
    "'''ML'''\n",
    "import prince\n",
    "import category_encoders as ce\n",
    "from sklearn import metrics, svm, preprocessing, utils\n",
    "from sklearn.metrics import mean_squared_error, r2_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,\\\n",
    "MaxAbsScaler,RobustScaler,QuantileTransformer,PowerTransformer\n",
    "from libsvm.svmutil import *\n",
    "\n",
    "# define set up types\n",
    "list_Set_Up = ['BL_Only','G_Only','G+BL','G+BL_Red','BL_Red_Only']\n",
    "# define degrees\n",
    "degrees = [4,5]\n",
    "\n",
    "# Defining main function \n",
    "def main():\n",
    "    final_out = []\n",
    "    df = pd.read_csv('files/output/baseline.csv',sep=';')\n",
    "    \n",
    "    # set the total iterations needed to 10\n",
    "    total_iterations = 10\n",
    "    \n",
    "    # for each iteration\n",
    "    for iteration in range(total_iterations):\n",
    "        out_dict = {}\n",
    "        out_dict['iteration'] = iteration\n",
    "        out_list = []\n",
    "        \n",
    "        # for each degree type\n",
    "        for n_deg in degrees:\n",
    "            # for each set up type\n",
    "            for setup_Type in list_Set_Up:\n",
    "                print(f\"\\nITERATION:{iteration}:DEGREE:{n_deg}:SETUP:{setup_Type} BEGIN...\")\n",
    "                    \n",
    "                #********* BASE LINE ONLY **********************************************************\n",
    "                if setup_Type == 'BL_Only':\n",
    "                    print(f\"\\nITERATION:{iteration}:DEGREE:{n_deg}:BASELINE_ONLY SET UP:START...\")\n",
    "                    # call the Baseline_Only module\n",
    "                    df_bo = Baseline_Only(df,n_deg, setup_Type, iteration)\n",
    "                    # drop the uuid\n",
    "                    df_bo = df_bo.drop(['uuid'],axis=1)\n",
    "                    df_simple = df_bo\n",
    "\n",
    "                    # pass the df to generate train/test split\n",
    "                    X_train,X_test,X,y,y_train,y_test,numeric_features,\\\n",
    "                    categorical_features = gen_Train_Test_Split(df_simple)\n",
    "\n",
    "                    # perform PCA and return PCA applied feature sets\n",
    "                    X_train,X_test = PCA_Industry(X_train,X_test)\n",
    "                    X_train,X_test = PCA_Country(X_train,X_test)\n",
    "                    #Visualize_Country_Ind_PCA(X,y)\n",
    "                    \n",
    "                    # Display the final train/test features shape\n",
    "                    print(\"Final train dataset shape\",X_train.shape)\n",
    "                    print(\"\\nFinal test dataset shape\",X_test.shape)                             \n",
    "                    \n",
    "                    # Run the classifer pipeline and get the results\n",
    "                    results = Run_Classifier(X_train,X_test,y_train,y_test,numeric_features,categorical_features,n_deg,setup_Type)\n",
    "                    \n",
    "                    # append results to the output list\n",
    "                    out_list.append(results)\n",
    "                    print(f\"\\nITERATION:{iteration}:DEGREE:{n_deg}:BASELINE_ONLY SET UP:END\")\n",
    "                \n",
    "                #********* GRAPH ONLY ********************************************************\n",
    "                elif setup_Type == 'G_Only':\n",
    "                    print(f\"\\nITERATION:{iteration}:DEGREE:{n_deg}:GRAPH_ONLY SET UP:START...\")\n",
    "                    \n",
    "                    # call the Graph_Only_SS module\n",
    "                    df_gr = Graph_Only_SS(df,n_deg, setup_Type, iteration)\n",
    "                    # drop the uuid\n",
    "                    df_gr = df_gr.drop(['uuid'],axis=1)\n",
    "                    df_simple = df_gr\n",
    "                    \n",
    "                    # pass the df to generate train/test split\n",
    "                    X_train,X_test,X,y,y_train,y_test,numeric_features,\\\n",
    "                    categorical_features = gen_Train_Test_Split(df_simple)\n",
    "                    \n",
    "                    # Display the final train/test features shape\n",
    "                    print(\"Final train dataset shape\",X_train.shape)\n",
    "                    print(\"\\nFinal test dataset shape\",X_test.shape)\n",
    "                    \n",
    "                    #print('\\nTest Dataframe Columns:\\n\\n{}'.format(X_test.columns.to_list()))\n",
    "                    # Run the classifer pipeline and get the results\n",
    "                    results = Run_Classifier(X_train,X_test,y_train,y_test,numeric_features,categorical_features,n_deg,setup_Type)\n",
    "                    \n",
    "                    # append results to the output list\n",
    "                    out_list.append(results)\n",
    "                    print(f\"\\nITERATION:{iteration}:DEGREE:{n_deg}:GRAPH_ONLY SET UP:END\")\n",
    "                \n",
    "                #********* GRAPH + BASELINE ONLY ***********************************************\n",
    "                elif setup_Type == 'G+BL':   \n",
    "                    print(f\"\\nITERATION:{iteration}:DEGREE:{n_deg}:GRAPH+BASELINE:START...\") \n",
    "                    \n",
    "                    # call the Graph_Only_SS module\n",
    "                    df_gr = Graph_Only_SS(df,n_deg, setup_Type, iteration)\n",
    "                    print(\"Graph shape after merge\",df_gr.shape)\n",
    "\n",
    "                    # call the Baseline_Only module\n",
    "                    df_bo = Baseline_Only(df,n_deg,'BL_Only', iteration)\n",
    "\n",
    "                    # merge the graphOnly feaures with baseline only features\n",
    "                    df_simple = pd.merge(df_gr.copy(),df_bo.copy(), how = 'inner',on='uuid')\n",
    "\n",
    "                    # drop unwanted columns\n",
    "                    df_simple = df_simple.drop(['uuid','p1_tag_y'],axis=1)\n",
    "                    print(\"Merged shape after baseline and graph\",df_simple.shape)\n",
    "                    # rename columns after merge\n",
    "                    df_simple = df_simple.rename(columns={\"p1_tag_x\": \"p1_tag\"})\n",
    "                    \n",
    "                    # pass the df to generate train/test split\n",
    "                    X_train,X_test,X,y,y_train,y_test,numeric_features,\\\n",
    "                    categorical_features = gen_Train_Test_Split(df_simple)\n",
    "                    \n",
    "                    # Display the final train/test features shape\n",
    "                    print(\"Before pca dataset shape\",X_train.shape)\n",
    "                    print(\"\\nBefore pca dataset shape\",X_test.shape)\n",
    "                    \n",
    "                    # perform PCA and return PCA applied feature sets\n",
    "                    X_train,X_test = PCA_Industry(X_train,X_test)\n",
    "                    X_train,X_test = PCA_Country(X_train,X_test)\n",
    "                    #Visualize_Country_Ind_PCA(X)\n",
    "                    \n",
    "                    # Display the final train/test features shape\n",
    "                    #print(\"Train set columns list\",X_train.columns)\n",
    "                    print(\"Final train dataset shape\",X_train.shape)\n",
    "                    print(\"\\nFinal test dataset shape\",X_test.shape)\n",
    "                    print('\\nTrain Dataframe Columns:\\n\\n{}'.format(X_train.columns.to_list()))\n",
    "                    #print('\\nTest Dataframe Columns:\\n\\n{}'.format(X_test.columns.to_list()))\n",
    "                    \n",
    "                    #check for nan and infinite columns if any\n",
    "                    nan_values = X_train.isna()\n",
    "                    nan_columns = nan_values.any()\n",
    "                    columns_with_nan = X_train.columns[nan_columns].tolist()\n",
    "                    if columns_with_nan != []:\n",
    "                        print(\"columns_with_nan \",columns_with_nan)\n",
    "                    print(\"Infinite columns train\",(X_train.columns.to_series()[np.isinf(X_train).any()]))\n",
    "                    print(\"Infinite columns test\",(X_test.columns.to_series()[np.isinf(X_test).any()]))\n",
    "                    \n",
    "                    # Run the classifer pipeline and get the results\n",
    "                    results = Run_Classifier(X_train,X_test,y_train,y_test,numeric_features,categorical_features,n_deg,setup_Type)\n",
    "                    \n",
    "                    # Append results to output list\n",
    "                    out_list.append(results)\n",
    "                    print(f\"\\nITERATION:{iteration}DEGREE:{n_deg}:GRAPH+BASELINE SET UP:END\")\n",
    "                \n",
    "                #********* GRAPH + BASELINE REDUCED ONLY *********************************************\n",
    "                elif setup_Type == 'G+BL_Red':\n",
    "                    print(f\"\\nITERATION:{iteration}:DEGREE:{n_deg}:GRAPH+BASELINE_REDUCED:START...\") \n",
    "                    \n",
    "                    # call the Graph_Only_SS module\n",
    "                    df_gr = Graph_Only_SS(df,n_deg, setup_Type, iteration)\n",
    "                    print(\"Graph shape after merge\",df_gr.shape)\n",
    "                    \n",
    "                    # call the Baseline_Reduced module\n",
    "                    df_bo = Baseline_Reduced(df,n_deg, 'BL_Red_Only', iteration)\n",
    "                    \n",
    "                    # merge the graphOnly feaures with baseline only features\n",
    "                    df_simple = pd.merge(df_gr.copy(),df_bo.copy(), how = 'inner',on='uuid')\n",
    "                    \n",
    "                    # drop unwanted columns\n",
    "                    df_simple = df_simple.drop(['uuid','p1_tag_y'],axis=1)\n",
    "                    print(\"Merged shape after baseline and graph\",df_simple.shape)\n",
    "                    #print(list(df_simple.columns))\n",
    "                    \n",
    "                    # rename columns after merge\n",
    "                    df_simple = df_simple.rename(columns={\"p1_tag_x\": \"p1_tag\"})\n",
    "                    \n",
    "                    # pass the df to generate train/test split\n",
    "                    X_train,X_test,X,y,y_train,y_test,numeric_features,\\\n",
    "                    categorical_features = gen_Train_Test_Split(df_simple)\n",
    "                    print(\"Before pca dataset shape\",X_train.shape)\n",
    "                    print(\"\\nBefore pca dataset shape\",X_test.shape)\n",
    "                    \n",
    "                    # perform PCA and return PCA applied feature sets\n",
    "                    X_train,X_test = PCA_Industry(X_train,X_test)\n",
    "                    X_train,X_test = PCA_Country(X_train,X_test)\n",
    "                    #Visualize_Country_Ind_PCA(X)\n",
    "                    print(\"Final train dataset shape\",X_train.shape)\n",
    "                    print(\"\\nFinal test dataset shape\",X_test.shape)\n",
    "                    print('\\nTrain Dataframe Columns:\\n\\n{}'.format(X_train.columns.to_list()))\n",
    "                    #print('\\nTest Dataframe Columns:\\n\\n{}'.format(X_test.columns.to_list()))\n",
    "                    \n",
    "                    #check for nan and infinite columns if any\n",
    "                    nan_values = X_train.isna()\n",
    "                    nan_columns = nan_values.any()\n",
    "                    columns_with_nan = X_train.columns[nan_columns].tolist()\n",
    "                    if columns_with_nan != []:\n",
    "                        print(\"columns_with_nan \",columns_with_nan)\n",
    "                    print(\"Infinite columns train\",(X_train.columns.to_series()[np.isinf(X_train).any()]))\n",
    "                    print(\"Infinite columns test\",(X_test.columns.to_series()[np.isinf(X_test).any()]))\n",
    "                    \n",
    "                    # Run the classifer pipeline and get the results\n",
    "                    results = Run_Classifier(X_train,X_test,y_train,y_test,numeric_features,categorical_features,n_deg,setup_Type)\n",
    "                    \n",
    "                    # Append results to output list\n",
    "                    out_list.append(results)\n",
    "                    print(f\"\\nDEGREE:{n_deg}:GRAPH+BASELINE_REDUCED SET UP:END\")               \n",
    "                \n",
    "                #********* BASELINE REDUCED ONLY *****************************************\n",
    "                elif setup_Type == 'BL_Red_Only':\n",
    "                    \n",
    "                    print(f\"\\nITERATION:{iteration}:DEGREE:{n_deg}:BASELINE_REDUCED_ONLY SET UP:START...\")\n",
    "                    \n",
    "                    # call the Baseline_Reduced module\n",
    "                    df_bo = Baseline_Reduced(df,n_deg, setup_Type, iteration)\n",
    "                    \n",
    "                    # drop unwanted columns\n",
    "                    df_bo = df_bo.drop(['uuid'],axis=1)\n",
    "                    df_simple = df_bo\n",
    "                    \n",
    "                    # pass the df to generate train/test split\n",
    "                    X_train,X_test,X,y,y_train,y_test,numeric_features,\\\n",
    "                    categorical_features = gen_Train_Test_Split(df_simple)\n",
    "                    \n",
    "                    # perform PCA and return PCA applied feature sets\n",
    "                    X_train,X_test = PCA_Industry(X_train,X_test)\n",
    "                    X_train,X_test = PCA_Country(X_train,X_test)\n",
    "                    #Visualize_Country_Ind_PCA(X,y)\n",
    "                    \n",
    "                    print(\"Final train dataset shape\",X_train.shape)\n",
    "                    print(\"\\nFinal test dataset shape\",X_test.shape)                             \n",
    "                    \n",
    "                    # Run the classifer pipeline and get the results\n",
    "                    results = Run_Classifier(X_train,X_test,y_train,y_test,numeric_features,categorical_features,n_deg,setup_Type)\n",
    "                    \n",
    "                    # Append results to output list\n",
    "                    out_list.append(results)\n",
    "                    print(f\"\\nDEGREE:{n_deg}:BASELINE_REDUCED_ONLY SET UP:END\")\n",
    "        \n",
    "        # append out_list values in an iteration to out_dict \n",
    "        out_dict['result'] = out_list\n",
    "        \n",
    "        # append out_dict to final_out\n",
    "        final_out.append(out_dict)\n",
    "        \n",
    "        # call Write_Output generate a json structure and store to a file\n",
    "        # for each iteration created\n",
    "        Write_Output(final_out,iteration)\n",
    "        print(f\"\\nITERATION:{iteration}:DEGREE:{n_deg}:END\")\n",
    "    #calculate_avg(total_iterations)\n",
    "    \n",
    "    #call the module generate_table to generate a output result df\n",
    "    # and display it for all the classifer\n",
    "    generate_table()\n",
    "    print(\"Completed all runs!....\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # execute only if run as a script\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63hRBi7U9ALk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7UXz7pA9ALk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "7_AM_Baseline_Graph_Combined_Model_Pipeline_100_Runs.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "common-cpu.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
